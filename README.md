# Big-Data-Fundamentals-with-PySpark
## Discription
Advance your data skills by mastering Apache Spark. Using the Spark Python API, PySpark, you will leverage parallel computation with large datasets, and get ready for high-performance machine learning. From cleaning data to creating features and implementing machine learning models, you'll execute end-to-end workflows with Spark. The track ends with building a recommendation engine using the popular MovieLens dataset and the Million Songs dataset.
### 1. Introduction to Big Data analysis with Spark
Fundamentals of BigData and introduction to Spark as distributed computing framework
* Main components: Spark Core and Spark built-in libraries - Spark SQL, Spark MLlib, Graphx, and Spark Streaming
* PySpark: Apache Spark's Python API to execute Spark jobs
* PySpark shell: For developing the interactive applications in python
* Spark modes: Local and clister mode
